{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba077eaf-4175-40fe-9deb-c173b48591ac",
   "metadata": {},
   "source": [
    "# Build a virtual assistant with Langchain and Zapier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc055f13-014b-46e6-b2e6-a2429fb79c45",
   "metadata": {},
   "source": [
    "In the realm of AI and automation, Langchain and Zapier stand as two exceptional tools, each with unique capabilities that can transform the way we work and interact. Langchain specializes in Conversational AI, enabling human-like conversations, while Zapier automates workflows between apps. When combined, they create a powerful synergy that gives birth to an intelligent virtual assistant capable of not just understanding and responding but also executing actions.\n",
    "\n",
    "**Langchain's Conversational AI**: Langchain brings lifelike conversations to the table, making interactions seamless and natural. It comprehends user queries, generates contextually relevant responses, and bridges the gap between humans and technology.\n",
    "\n",
    "**Zapier's Automation Magic**: Zapier is the master of automation, connecting apps and enabling them to collaborate effortlessly. It triggers actions in one app based on events in another, streamlining tasks and minimizing manual effort.\n",
    " \n",
    "By merging Langchain's conversational prowess with Zapier's automation muscle, you create a virtual assistant that does more than just chat. It performs tasks, gathers information, and executes actions across multiple platforms, all while engaging users in meaningful conversations.\n",
    "\n",
    "Imagine your virtual assistant not only responding intelligently to inquiries but also performing tasks like searching Google for information and emailing the results. Langchain's understanding meets Zapier's execution, seamlessly bridging the gap between conversation and action.\n",
    "\n",
    "In this blog, i will show you how to set up Langchain and Zapier together to build a virtual assistant :). Let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f4f74-dadd-4952-8153-c9c1c04d7b0a",
   "metadata": {},
   "source": [
    "## Proccess to build a virtual assistant\n",
    "\n",
    "BUilding a virtual assistant has quite lots of steps to do.\n",
    "\n",
    "1. **Setup**: Obtain API keys for Langchain and Zapier.\n",
    "2. **Design**: Define assistant's tasks and interactions.\n",
    "3. **Langchain**: Initialize `OpenAI`, create tools, and set up the agent.\n",
    "4. **Zapier**: Initialize `ZapierNLAWrapper`, define actions.\n",
    "5. **Input**: Record and process user input, like voice or text.\n",
    "6. **Response**: Use Langchain to generate assistant's response.\n",
    "7. **Actions**: Trigger Zapier workflows for actions.\n",
    "8. **Display**: Show assistant's response to the user.\n",
    "9. **Test**: Thoroughly test and refine the assistant.\n",
    "10. **Deploy**: Deploy and gather user feedback for improvements.\n",
    "\n",
    "This fusion empowers the assistant to converse naturally while seamlessly automating tasks, creating an efficient and user-friendly virtual companion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4e7a9-64c2-40ad-b794-29d3e76517c7",
   "metadata": {},
   "source": [
    "## Build a primative virtual assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71df39-ef82-4e8c-89c5-60d008978f91",
   "metadata": {},
   "source": [
    "First, we will need to import the necessary modules to build the virtual assistant. Langchain has a helpful [documentation](https://python.langchain.com/docs/integrations/tools/zapier) on this so make sure to check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36948489-66c6-4bdb-872f-474d435e2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.agents.agent_toolkits import ZapierToolkit\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.tools import BaseTool, Tool\n",
    "from langchain.utilities.zapier import ZapierNLAWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce8193-a7a5-4051-8f8d-5ddc13b62764",
   "metadata": {},
   "source": [
    "We will need [OpenAI API key](https://platform.openai.com/account/api-keys) and [Zapier API key](https://nla.zapier.com/credentials/). THe keys are essential to access OpenAI and Zpaier's functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1957327b-6d38-4ac4-b1a1-67dd04fe2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY =  \"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\"\n",
    "ZAPIER_NLA_API_KEY = \"sk-ak-RNh2rJkRV05OXwMlAl2taw8OGE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45f9cf-d611-4293-b83b-fed27403f034",
   "metadata": {},
   "source": [
    "Now, we will need to set up the Langchain Language Model, a conversation memory, the Zapier NLA Wrapper, and the Zapier toolkit. This foundation enables your virtual assistant to communicate effectively and perform various actions through the integration of Langchain and Zapier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d5d61bad-ee56-48fd-b0d5-8a2a1ab351db",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, openai_api_key= OPENAI_API_KEY)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "zapier = ZapierNLAWrapper(zapier_nla_api_key = ZAPIER_NLA_API_KEY)\n",
    "toolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f849ebc-250c-444e-a7d0-3bfb6a365475",
   "metadata": {},
   "source": [
    "We will need to define `tools` for the Virtual Assistant to use as well. `tools` will include the tools defined in your Zapier toolkit along with the \"human\" tool from Langchain's toolset. These tools collectively define the capabilities of your virtual assistant, allowing it to interact with users, perform actions through Zapier, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d429871-625a-4d83-8014-5af1367b9d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = toolkit.get_tools() + load_tools([\"human\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bfe41c-8a36-43a1-b9d8-8631640ff7a4",
   "metadata": {},
   "source": [
    "We will need to set up an agent. An agent is a crucial component in building an effective virtual assistant with Langchain and Zapier. It acts as the conversational orchestrator, ensuring smooth interactions between users and the system. The agent is responsible for processing user inputs, generating coherent responses using Langchain's Language Model, and seamlessly integrating various tools from the Zapier toolkit to perform tasks like web searches, sending emails, and more. Beyond managing conversations, the agent can also learn and adapt based on user interactions, enhancing the quality of its responses over time. By serving as the central hub that connects user inputs, language generation, and tool execution, the agent plays a pivotal role in creating a powerful and efficient virtual assistant experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46efab72-7083-4806-8643-1535443289c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f500ba0f-e232-4fea-b019-20059c9a30d8",
   "metadata": {},
   "source": [
    "## Set up custom tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd45eb3-3027-4af2-8743-e9ed3a89db6f",
   "metadata": {},
   "source": [
    "That was a pretty good virtual assistant that we have set up there. However, I want the VA to have more functionality, fr example it can go on the internet and search for some information and then send an email the result. Let's upgrade our VA.\n",
    "Im going to create Google Search tools for our VA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60922558-5d25-4218-923c-3c4c719f5df3",
   "metadata": {},
   "source": [
    "First, we need to set up the proper API keys and environment variables. To set it up, create the GOOGLE_API_KEY in the [Google Cloud credential console](https://console.cloud.google.com/apis/credentials) and a GOOGLE_CSE_ID using the [Programmable Search Engine](https://programmablesearchengine.google.com/controlpanel/create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "191cb696-3dfe-460f-9762-9856bcd3e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"AIzaSyDb9greO_WtpGTu2n96k-ucSCQ6TSU0-FY\"\n",
    "GOOGLE_CSE_ID = \"c188eddc1df614f10\"\n",
    "SERPER_API_KEY = \"b2ccc4a941fed316b212f74f1485a0255b6c7962\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb79f14-0e86-4c91-a911-4982d8daee43",
   "metadata": {},
   "source": [
    "To create custom tools, Langchain have a variety of ways to do it. For anyone who wishes to learn more, you can read this Langchain's document about creating custom tools. Im going to create functions using BaseTool method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7eac2976-cbea-4cfc-9d94-91644fa5b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Type\n",
    "\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92233a59-cdbb-4998-ba1f-c5aeb6b474b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleSearchTool(BaseTool):\n",
    "    name = \"Google Search\"\n",
    "    description = \"Use this tool to search on Google.\"\n",
    "\n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        search = GoogleSearchAPIWrapper(google_api_key=GOOGLE_API_KEY, google_cse_id=GOOGLE_CSE_ID)\n",
    "        return search.run(query)\n",
    "\n",
    "    async def _arun(\n",
    "        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"Google Search does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9739b6c6-b78b-4896-8dac-2879d4a9588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleSerperTool(BaseTool):\n",
    "    name = \"Google Serper\"\n",
    "    description = \"Useful for when you need to ask with search\"\n",
    "\n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        search = GoogleSerperAPIWrapper(serper_api_key=SERPER_API_KEY)\n",
    "        return search.run(query)\n",
    "\n",
    "    async def _arun(\n",
    "        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"Google Serper does not support async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d4c976-53f6-4b42-b5da-9a2775457a04",
   "metadata": {},
   "source": [
    "## Recording voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f406a58-16cc-43d1-84a7-55ac17a6eaeb",
   "metadata": {},
   "source": [
    "So we have the basic functionalities of our virtual assistant, next we can set up a voice recording functionality which is totally optional, but it will be a great way for us to communicate with our virtual assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d8978a7-63c2-484f-8c29-a3cfc2553f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import pyttsx3\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5faee4a-dc34-47fe-b42d-70af9630d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 5  # duration of each recording in seconds\n",
    "fs = 44100  # sample rate\n",
    "channels = 1  # number of channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65952eac-83fb-41a3-a356-1c23555956ed",
   "metadata": {},
   "source": [
    "We define a `record_audio` function, this function captures audio for a specified duration, sample rate, and number of channels, using the `sounddevice` library, and then returns the recorded audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "89667216-bf0e-4593-b712-f5e72f734453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(duration, fs, channels):\n",
    "    print(\"Recording...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=channels)\n",
    "    sd.wait()\n",
    "    print(\"Finished recording.\")\n",
    "    return recording"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdfee40-88f0-4e31-beec-5f37a1b529e7",
   "metadata": {},
   "source": [
    "The `transcribe_audio` function is a crucial part of the audio processing pipeline. It takes in recorded audio data and the associated sample rate as input. The process involves temporarily saving the audio data to a file, utilizing OpenAI's Audio API to transcribe the audio into text by specifying the appropriate model, and then cleaning up by removing the temporary audio file. The transcribed text is extracted from the API response and returned as the result.\n",
    "There are multiple APIs to use to transcribe the audio, I use OpenAI's API cause it's convenient. But use can also use Google Cloud's API or any other APIs to transcribe as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb50c472-e407-41de-be3e-5f2fa42444d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(recording, fs):\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_audio:\n",
    "        sf.write(temp_audio.name, recording, fs)\n",
    "        temp_audio.close()\n",
    "        #openai.api_key = OPENAI_API_KEY\n",
    "        with open(temp_audio.name, \"rb\") as audio_file:\n",
    "            transcript = openai.Audio.transcribe(\"whisper-1\", audio_file, openai_api_key = OPENAI_API_KEY)\n",
    "\n",
    "        os.remove(temp_audio.name)\n",
    "\n",
    "    return transcript[\"text\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7b339-eede-4589-a1e1-b01379c0d45e",
   "metadata": {},
   "source": [
    "Last, let's define a `speak` function so that the Virtual Assistant can reply to us by audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b35742bf-7800-46e5-baac-ddaaf5de432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53377a7-5e30-4715-ac3f-ace87b87c423",
   "metadata": {},
   "source": [
    "## Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b9a4c-00ce-4efa-b166-0bb50eb95a26",
   "metadata": {},
   "source": [
    "Now, we have everything, let's define the tools that our VA will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3af5e2b3-2ee0-4866-9cf8-b97a6556c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    GoogleSearchTool(),\n",
    "    GoogleSerperTool(),\n",
    "] + toolkit.get_tools() + load_tools([\"human\"])\n",
    "\n",
    "agent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ecf4e1-ede0-4dbc-bca2-20fb08aefd9e",
   "metadata": {},
   "source": [
    "And set up a loop so that we can communicate with our VA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3313d-3cf4-4803-9a16-c63f81e3afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(\"Press Enter to start recording.\")\n",
    "    input()  # Wait for Enter key\n",
    "    recorded_audio = record_audio(duration, fs, channels)\n",
    "    message = transcribe_audio(recorded_audio, fs)\n",
    "    print(f\"You: {message}\")\n",
    "    assistant_message = agent.run(message)\n",
    "    speak(assistant_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388eb3ef-8aa9-44ba-9ded-c1ea338bc2e9",
   "metadata": {},
   "source": [
    "As we draw the curtains on this virtual assistant journey, the delightful partnership between Langchain and Zapier takes center stage. What's the magic behind it all, you ask? Well, imagine a virtual assistant that not only answers your questions but also performs Google searches and effortlessly transcribes audio inputs. That's the power of combining Langchain's conversational finesse with Zapier's automation prowess.\n",
    "\n",
    "Langchain, the mastermind behind managing conversational agents, memory, and tools, brings a touch of intelligence to the table. On the other hand, Zapier, the automation wizard, weaves its magic by connecting various services and actions seamlessly. \n",
    "\n",
    "Whether you're looking to enhance user interactions or streamline tasks, this collaboration has your back. And for the curious minds, the [GitHub repository]() holds the key to unraveling the intricacies of this partnership. \n",
    "\n",
    "See you in the next tutorial!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
